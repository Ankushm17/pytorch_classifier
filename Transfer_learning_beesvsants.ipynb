{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_learning_beesvsants.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpzK7UwcgDs"
      },
      "source": [
        "Following classification model classifies bees and ants using resnet50 pretrained on imagenet dataset. The following code is in reference to the computer vision transfer learning tutorial from pytorch documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O5SHN5udKfT"
      },
      "source": [
        "Importing the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBwWyWv3yPMU"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ4d62stdTID"
      },
      "source": [
        "Data augmentation usind torchvision.transforms class provided by pytorch.\n",
        "We are cropping the image to 224 px as the pretrained model accepts the minimum size of 224 px, horizontally flipping it and converting into tensors followed by normalizing the tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH2tAkRyyZBC"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0buqpnmeGAN"
      },
      "source": [
        "The bees vs ants data was loaded into the drive which is mounted to pipe the data into the dataloaders. We are iterating through the dataset to apply transformations to the images and loading the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvkjID-dyi7c"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/Data/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEdM4PpzeuMj"
      },
      "source": [
        "The following code is for training the model, we copy the model weights in best_model_wts and compare it with the weights for other iterations. The set of weights which give the highest accuracy are used for the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzOz1WbwztOy"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=8):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "       \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  \n",
        "            else:\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "           \n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "               \n",
        "                optimizer.zero_grad()\n",
        "\n",
        "               \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsCsJ-8afg09"
      },
      "source": [
        "The following code imports the resnet50 pretrained model from torchvision.models, we do not apply the gradient and add a linear layer in the end to get a binary output. We apply stochastic gradient descent only on the fully connected layer. We are also using the lr_sceduler to decay the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdpBRKX5z5kA"
      },
      "source": [
        "model_conv = torchvision.models.resnet50(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoHRi8HugfAl"
      },
      "source": [
        "Training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyW_6cHc0KCF",
        "outputId": "2fbefe4a-a7e4-40d5-e7a2-fb43c31ebf51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/7\n",
            "----------\n",
            "train Loss: 0.6757 Acc: 0.6639\n",
            "val Loss: 0.6022 Acc: 0.6797\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 0.5492 Acc: 0.7664\n",
            "val Loss: 0.2393 Acc: 0.9085\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 0.4973 Acc: 0.7746\n",
            "val Loss: 0.2580 Acc: 0.8824\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 0.4623 Acc: 0.7787\n",
            "val Loss: 0.2175 Acc: 0.9085\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 0.2480 Acc: 0.8975\n",
            "val Loss: 0.2853 Acc: 0.8758\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 0.4520 Acc: 0.8156\n",
            "val Loss: 0.2301 Acc: 0.9281\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 0.3418 Acc: 0.8607\n",
            "val Loss: 0.1663 Acc: 0.9346\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 0.2894 Acc: 0.8648\n",
            "val Loss: 0.1511 Acc: 0.9346\n",
            "\n",
            "Training complete in 12m 27s\n",
            "Best val Acc: 0.934641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo4YvAjXgiY8"
      },
      "source": [
        "Saving the model using torch.save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQVoNk909FK4"
      },
      "source": [
        "PATH = '/content/drive/My Drive/BEES_ANTS.pt'\n",
        "torch.save(model_conv, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsKjvUW-9ebz"
      },
      "source": [
        "#model_conv = torch.load(PATH)\n",
        "model_conv.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-HfcgFPgrsL"
      },
      "source": [
        "The following code helps to  visualize the performance of the model. We are loading some images from the validation dataset and passing through the model and displaying the obtained result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNT1ZS8ukYoW"
      },
      "source": [
        "def visualize_model(model, num_images=5):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpjeJD_khjlE"
      },
      "source": [
        "Passing model_conv to visualize_model function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccwPrjJnm2BZ"
      },
      "source": [
        "visualize_model(model_conv)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}